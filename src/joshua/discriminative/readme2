

1) Supervised Min Risk training.

a)Parameters set for discriminative training are as follows:
(note HG stands for hypergraph)

discriminative aexperiment/featureFile 1.0 ; sparse features
maxNumIter=10    		; Number of iterations used in MinRisk training. 
useSemiringV2=true		; Use V2 semiring for training on HG, no need to change
maxNumHGInQueue=100		; Number of HG stored in the Producer for the Producer-consumer model. The larger the number, the more memory used, the faster the training. 
numThreads=40			; Number of parallel threads in the Producer-consumer model.
saveHGInMemory=false		; ?, no need to change.
printFirstN=10			; print the first N gradients and weights.

#option for first feature (e.g., baseline feature)
normalizeByFirstFeature=true	; as it says
fixFirstFeature=false		; as it says

#loss-augmented pruning
lossAugmentedPrune=false	; ? 
startLossScale=10		; ?
lossDecreaseConstant=2		; ?


#google linear corpus gain
useGoogleLinearCorpusGain=true
#googleBLEUWeights=-1.0;0.10277777076514476;0.07949965001350584;0.6993000659479868;0.09565585699195878
googleBLEUWeights=-1.0;0.3333333333333333;0.584795321637427;1.0259567046270646;1.7999240432053771
; Weights for the linear combination of N-gram precisions, details are explained later. 

#annealing?
annealingMode=0			; 1 stands for use determined annealing, 0 stand for not use or use temperature 0. 
isScalingFactorTunable=false	; no need to change
useL2Regula=true		; use L2 regularization or not
varianceForL2=1			; 1/m * ||w||^2
useModelDivergenceRegula=false	; No need to change
lambda=-1			; 

#feature related
#dense features
useBaseline=false		; Use a singel baseline model, all baseline features are combined together, as f_base = \sum w_i*f_i, then combined with sparse featres as F*W = w_base*f_base + W_sp*F_sp. w_base and W_sp are tuned.
baselineFeatureName=baseline_lzf ;
baselineFeatureWeight=1.0	 ; weight w_base for f_base

useIndividualBaselines=true	; Use baseline features individualy as F*W = W_base*F_base + W_sp*F_sp, W_base, and W_sp are tuned.
baselineFeatIDsToTune=0;1;2;3;4;5;6;7;8;9  ; Specify the individual baseline features in W_base to be tuned, other features are fixed.

#sparse features
useSparseFeature=false		; Use sparse features

useRuleIDName=true		; no need to change

useTMFeat=true			; Sparse feature types
useTMTargetFeat=false

useMicroTMFeat=true
wordMapFile=src/joshua/discriminative/training/risk_annealer/data/wordMap

useLMFeat=true
startNgramOrder=1
endNgramOrder=2

b) A common supervised training procedure is as following, 
i) Prepare bilingual text data as insturcted by Chris's tutorial
http://www.cs.jhu.edu/~ccb/joshua/index.html
Perform every step before step 6, MERT.

ii) Three parameters should be set according to the data: 
lm_file= 			Language model
tm_file=			Extracted grammars 
googleBLEUWeights= 		This weights are generated by the data stats
Use: 
java -Djava.library.path=/home/zwang40/joshua/lib -d64 -Xmx100g -cp ~/joshua/bin/ joshua.discriminative.bleu_approximater.ComputeLinearCorpusGainThetas numUnigramTokens unigramPrecision decayRatio 
decayRatio=1/4(1gram_precision/2gram_precision + 3gram_precision/2gram_precision + 4gram_precision/3gram_precision)

These precisions can be achieved during a dryrun without any tuning. 

iii) Use joshua_mr.config as the configuration file, then run the MinRisk training : 
nohup $JAVA_HOME/bin/java -Djava.library.path=/home/zwang40/joshua/lib -d64 -Xmx120g -cp ~/joshua/bin/ joshua.discriminative.training.risk_annealer.hypergraph.HGMinRiskDAMert mr/joshua.config dev/set1.f mr/hg dev/set1.e*

Here set1.f is the foreign text and set1.e* are their reference. Those together should be another development dataset other than the training corpora.


This will trigger a training program, that generates the following files:
mr/hg.[1-maxIter] Kbest lists generated at each iteration's decoding step.
mr/hg.[1-maxIter].hg.items HGs generated at each iteration's decoding step.
mr/hg.[1-maxIter].hg.rules Grammar rules used at each iteration's decoding step
mr/joshua_mr.config.[1-maxIter]. Joshua configuration files after each iteration

And
mr/joshua_mr.final the training output configuration file.

iv) Change tm_file to the grammar file of the test set, and use that configration file for decoding.

b) Unsupervised Contrastive Estimation (CE) training 
This training paradigm relies on the confusion grammar(CG) and the previous supervised MinRisk training.
Datasets: set0.f       set0.e       bilingual training corpus
	  set1	       set1.e[1-n]   bilingual development corpus
	  set1p	       		     monolingual development corpus
	  set2	       set2.e[1-k]   testset

i) Combine set1 and set2, extracted a single grammar from that combined file(dev/grammar.set12)
ii) Extract CG file from grammar.set12

java -Xmx20g -cp /home/zwang40/joshua/bin/ -Djava.library.path=/home/zwang40/joshua/lib -Dfile.encoding=utf8 joshua.discriminative.training.contrastive_estimation.ConfusionDeriver dev/grammar.set12 confusion/grammar.confuse 1 1 1

iii) Use the previous supervised training setting and modify the following fields:
tm_file=confusion/grammar.confuse
Use only LM and Wordpenalty feature, plus the sparse features.

iv) Run the supervised MinRisk training using CG (CG MinRisk)
First prepare sparse features that we want to tune using CG. Generally, these features should be restricted to the english side of the translation and form a discriminative language model(log-linear model), which we call it a CE language model(CELM).

nohup $JAVA_HOME/bin/java -Djava.library.path=/home/zwang40/joshua/lib -d64 -Xmx120g -cp ~/joshua/bin/ joshua.discriminative.training.risk_annealer.hypergr
aph.HGMinRiskDAMert confuse/joshua.config dev/set1p confuse/hg dev/set1p


This training process will use CG to build a contrastive neighborhood set around the monolingual english text, and trying to learn the discriminative language model weights(CELM), which are sparse features in our translation model. Thus, features tuned in this process need to be restricted to the english side of the training output. 


v) Run the supervised MinRisk training using the CELM
nohup $JAVA_HOME/bin/java -Djava.library.path=/home/zwang40/joshua/lib -d64 -Xmx100g -cp ~/joshua/bin/ joshua.discriminative.training.risk_annealer.hypergraph.HGMinRiskDAMert mr/joshua.config dev/set1.f mr/hg dev/set1.e*  

Use the previous section's MinRisk training setting and dataset set1, modify the following fields.

discriminative confusion/feat.final f_CELM # Set this to the final feature file tuned via the CG MinRisk training.
useSparseFeature=false		    # the sparse features are already tuned by CG MinRisk, and fixed here 

In principle, this training process is to combine the CELM(sparse features) with other baseline features(dense features). We treat CELM as a singel baseline feature, and tune its weight(CE_LM) together with other baseline features.

vi) Use the final model to decode.
