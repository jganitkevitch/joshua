lm_file=C:/data_disk/java_work_space/sf_trunk/example2/example2.4gram.lm.gz
#lm_file=C:/data_disk/java_work_space/sf_trunk/example/example.trigram.lm.gz

tm_file=C:/data_disk/java_work_space/sf_trunk/example2/example2.hiero.tm.gz
tm_format=hiero

glue_file=C:/data_disk/java_work_space/sf_trunk/grammars/hiero.glue
glue_format=hiero

#lm config
use_srilm=false
lm_ceiling_cost=100
use_left_euqivalent_state=false
use_right_euqivalent_state=false
order=4


#tm config
span_limit=10
phrase_owner=pt
mono_owner=mono
begin_mono_owner=begin_mono
default_non_terminal=X
goalSymbol=S

#pruning config
useCubePrune=true
useBeamAndThresholdPrune=true
fuzz1=0.1
fuzz2=0.1
max_n_items=30
relative_threshold=10.0
max_n_rules=50
rule_relative_threshold=10.0

#nbest config
use_unique_nbest=true
use_tree_nbest=false
add_combined_cost=true
top_n=300


#remoter lm server config,we should first prepare remote_symbol_tbl before starting any jobs
use_remote_lm_server=false
remote_symbol_tbl=./voc.remote.sym
num_remote_lm_servers=4
f_remote_server_list=./remote.lm.server.list
remote_lm_server_port=9000


#parallel deocoder: it cannot be used together with remote lm
num_parallel_decoders=1
parallel_files_prefix=.

#disk hg
save_disk_hg=true
forest_pruning=false
forest_pruning_threshold=150


###### model weights
#lm order weight
lm 1.000000

#phrasemodel owner column(0-indexed) weight
phrasemodel pt 0 1.066893
phrasemodel pt 1 0.752247
phrasemodel pt 2 0.589793

#arityphrasepenalty owner start_arity end_arity weight
#arityphrasepenalty pt 0 0 1.0
#arityphrasepenalty pt 1 2 -1.0

#phrasemodel mono 0 0.5

#wordpenalty weight
wordpenalty -2.844814
#latticecost 1.0


#========================discriminative model options
#discriminative C:\data_disk\java_work_space\discriminative_at_clsp\edu\jhu\joshua\training\risk_annealer\data\featureFile 1.0

#general
oneTimeHGRerank=false
maxNumIter=1
useSemiringV2=true
maxNumHGInQueue=20
numThreads=20
saveHGInMemory=false


#option for first feature (e.g., baseline feature)
normalizeByFirstFeature=true
fixFirstFeature=false


#google linear corpus gain
useGoogleLinearCorpusGain=true
unigramPrecision=0.85
precisionDecayRatio=0.7
numUnigramTokens=10

#annealing?
noAnnealing=true
isScalingFactorTunable=false

#feature related
#dense features
useBaseline=false
baselineFeatureName=baseline_lzf
baselineFeatureWeight=1.0

useIndividualBaselines=true
numIndividualBaselines=5

#sparse features
useSparseFeature=false
useTMFeat=true
useTMTargetFeat=false

useLMFeat=true
startOrder=1
endOrder=2





